<!-- 1. Load Modules-->
<section>
    <h1>
      Load Modules
    </h1>
    <p>
      We load our modules into our python environment. In my case I am employing a **Jupyter Notebook** while running inside a **conda** environment. 

      For now to illustrate and show the module versions in a simple way I will name the ones I used and show the version I used as follows:
      <ul>
        <li> 
          numpy==1.11.2
        </li>
        <li>
          pandas==0.18.1
        </li>
        <li>
          matplotlib==1.5.3
        </li>
        <li>
          sklearn==0.18.1
        </li>
        <li>
          seaborn=0.7.1
        </li>
      </ul>
    </p>
<!-- CODE HERE -->

    <pre style="color:white">
      %matplotlib inline

      import numpy as np
      import pandas as pd <strong><span style="color:#348589"> # Data frames</span></strong>
      import matplotlib.pyplot as plt <strong><span style="color:#348589"> # Visuals</span></strong>
      import seaborn as sns <strong><span style="color:#348589"># Danker visuals</span></strong>
      from sklearn.model_selection import train_test_split <strong><span style="color:#348589"># Create training and test sets</span></strong>
      from sklearn.model_selection import KFold, cross_val_score <strong><span style="color:#348589"># Cross validation </span></strong>
      from sklearn.neighbors import KNeighborsClassifier <strong><span style="color:#348589"># Kth Nearest Neighbor</span></strong>
      from sklearn.tree import DecisionTreeClassifier <strong><span style="color:#348589"># Decision Trees</span></strong>
      from sklearn.tree import export_graphviz <strong><span style="color:#348589"># Extract Decision Tree visual</span></strong>
      from sklearn.ensemble import RandomForestClassifier <strong><span style="color:#348589"># Random Forest</span></strong>
      from sklearn.neural_network import MLPClassifier <strong><span style="color:#348589"># Neural Networks</span></strong>
      from sklearn.metrics import roc_curve <strong><span style="color:#348589"># ROC Curves</span></strong>
      from sklearn.metrics import auc <strong><span style="color:#348589"># Calculating Area Under Curve for ROC's!</span></strong>
      from urllib.request import urlopen <strong><span style="color:#348589"># Get data from UCI Machine Learning Repository</span></strong>
      
      pd.set_option('display.max_columns', 500) <strong><span style="color:#348589"># Included to show all the columns
      # since it is a fairly large data set</span></strong>
      plt.style.use('ggplot') <strong><span style="color:#348589"># Using ggplot2 style visuals because that's how I learned my visuals 
      # and I'm sticking to it!</span></strong>
    </pre>
<!-- [END] CODE HERE -->
<!-- CODE HERE -->
        
    <p>Next we load our data.</p>
</section>

<!-- 2. Load Data-->
<section>
    <h1>
      Load Data
    </h1>
    <p>
      For this section, I'll load the data into a <strong>Pandas</strong> dataframe using `urlopen` from the `urllib.request` module. 
      
      Instead of downloading a csv, I started implementing this method(Inspired by Jason's Python Tutorials) where I grab the data straight from the <strong>UCI Machine Learning Database</strong>. Makes it easier to go about analysis from online sources and cuts out the need to download/upload a csv file when uploading on <strong>GitHub</strong>. I create a list with the appropriate names and set them within the dataframe. <strong>NOTE</strong>: The names were not documented to well so I used <a href="https://www.kaggle.com/buddhiniw/d/uciml/breast-cancer-wisconsin-data/breast-cancer-prediction">this analysis</a> to grab the variable names and some other tricks that I didn't know that were available in <strong>Python</strong> (I will mention the use in the script!)
      
      Finally I set the column `id_number` as the index for the dataframe. 
    </p>
    
<!-- CODE HERE -->
    <pre style="color:white"><samp>attach(iris)
data(iris)
head(iris)
</samp></pre>
<h2>Terminal Output</h2>
<pre><span>> head(iris)
Sepal.Length Sepal.Width Petal.Length Petal.Width Species
1          5.1         3.5          1.4         0.2  setosa
2          4.9         3.0          1.4         0.2  setosa
3          4.7         3.2          1.3         0.2  setosa
4          4.6         3.1          1.5         0.2  setosa
5          5.0         3.6          1.4         0.2  setosa
6          5.4         3.9          1.7         0.4  setosa
</span></pre>
    
    <p>Our terminal output above shows six observations of our data. We can appreciate a total of five variables. The goal is to predict <em>species</em> as a function of the other four variables.</p>

<!-- [END] CODE HERE -->
    
    <p>Next we do exploratory analysis.</p>
</section>
<!-- 3. Exploratory Analysis-->
<section>
    <h1>Exploratory Analysis</h1>
    <p>We begin our exploratory analysis by looking for relationships across our explanatory and predicted variables. For this, we use <code>ggplot()</code> and <code>plotly</code> to generate scatterplots of the <code>sepal length</code> (y-axis) and <code>sepal width</code> (x-axis), and the <code>petal length</code> (y-axis) and <code>petal width</code> (x-axis).</p>
    
<!-- CODE HERE -->
    <pre style="color:white"><samp>gg1<-ggplot(iris,aes(x=Sepal.Width,y=Sepal.Length, 
                     shape=Species, color=Species)) + 
  theme(panel.background = element_rect(fill = "gray98"),
        axis.line   = element_line(colour="black"),
        axis.line.x = element_line(colour="gray"),
        axis.line.y = element_line(colour="gray")) +
  geom_point(size=2) + 
  labs(title = "Sepal Width Vs. Sepal Length")

ggplotly(gg1)</samp></pre>

<iframe width="100%" height=415  frameborder="0" scrolling="no" src="https://plot.ly/~raviolli77/63.embed?autosize=True&width=90%&height=100%"></iframe>
<p>Next we plot the <code>petal length</code> vs the <code>petal width!</code></p>
<pre style="color:white"><samp>gg2<-ggplot(iris,aes(x=Petal.Width,y=Petal.Length, 
                     shape=Species, color=Species)) + 
  theme(panel.background = element_rect(fill = "gray98"),
        axis.line   = element_line(colour="black"),
        axis.line.x = element_line(colour="gray"),
        axis.line.y = element_line(colour="gray")) + 
  geom_point(size=2) + 
  labs(title = "Petal Length Vs. Petal Width")

ggplotly(gg2)</samp></pre>

<iframe width="100%" height=415  frameborder="0" scrolling="no" src="https://plot.ly/~raviolli77/61.embed?autosize=True&width=90%&height=100%"></iframe>


<!-- [END] CODE HERE -->
    
    <p>
       The plots below shows setosa to be most distinguisable of the three species with respect to both sepal and petal attributes. We can infer then that the setosa species will yield the least prediction errors, while the other two species, versicolor and virginica, might not.
    </p>
    
    <p>Below is a plot that shows the relationships across our various explanatory variables.</p>
    <pre style="color:white"><samp>pairs <- ggpairs(iris,mapping=aes(color=Species),columns=1:4) + 
  theme(panel.background = element_rect(fill = "gray98"),
        axis.line   = element_line(colour="black"),
        axis.line.x = element_line(colour="gray"),
        axis.line.y = element_line(colour="gray")) 
pairs
ggplotly(pairs) %>%
  layout(showlegend = FALSE)</samp></pre>
  
<iframe width="100%" height=415  frameborder="0" scrolling="no" src="https://plot.ly/~raviolli77/65.embed?autosize=True&width=90%&height=100%"></iframe>
<p>This plot reduces the dimensions and gives an overarching view of the interactions of the different attributes. This plot will be handy for other classification models like <strong>Linear Discrimant Analysis</strong> which is not in this project but we included more statistical process on <strong>Iris</strong> in the Github respository.</p>



    </section>

<!-- 4. KNN Modeling-->
<section>
    <h1>Model Estimation</h1>
    <p>The K-Nearest Neighbor algorithm predicts based on majority votes, measuring a certain number of neighboring observation points (k) and classifies based on attribute prevalence using Euclidean distance. Here is <a href="https://stat.ethz.ch/R-manual/R-devel/library/class/html/knn.html">documentation</a> on the <code>knn()</code> method. Check the <a href="https://cran.r-project.org/web/packages/class/class.pdf">documentation</a> on its package<code>CLASS</code> as well. </p>
    
    <p>We begin this section by creating training and test sets with 75% and 25% of observations generated randomly hence the <code>set.seed</code> function. Here is a nice <a href="http://stackoverflow.com/questions/17200114/how-to-split-data-into-training-testing-sets-using-sample-function-in-r-program">stack overflow post</a> on how to split data into training and testing sets. We didn't post the outputs of the indices or the sets because it would be too much code outputted so we recommend to run for yourself to understand what's going on! Once you run the code you'll get a pretty good idea of what we're doing.</p>
    
<!-- CODE HERE -->
    <pre style="color:white"><samp><strong><span style="color:#348589"># Creating training/test set</span></strong> 
set.seed(123)
samp.size <- floor(nrow(iris) * .75)
samp.size
set.seed(123)

train.ind <- sample(seq_len(nrow(iris)), size = samp.size) 
train.ind 
train <- iris[train.ind, ] 
head(train)

train.set <- subset(train, select = -c(Species))
head(train.set)

test <- iris[-train.ind, ]
head(test)

test.set <- subset(test, select = -c(Species))
head(test.set)

class <- train[,"Species"]

test.class <- test [,"Species"]</samp></pre>
    
    <p>
        Before entering parameter values into the <code>knn()</code> method we use cross validation from the <code>caret</code> package to find the optimal value for <strong>k</strong>. This optimal <strong>k</strong> will help us produce the smallest test error rate. The <code>trainControl()</code> and <code>train()</code> methods help us with this task.
    </p>
    
    <pre style="color:white"><samp>set.seed(123)

contrl <- trainControl(method="repeatedcv",repeats = 3)

knn.K <- train(Species ~ ., data = train, method = "knn", trControl = contrl, preProcess = c("center","scale"),tuneLength = 20)


<strong><span style="color:#348589"># Here we output the results from knn.K! </span></strong>
knn.K
</samp></pre>
<h2>Terminal Output</h2>
<pre><samp>> knn.K
k-Nearest Neighbors 

112 samples
  4 predictors
  3 classes: 'setosa', 'versicolor', 'virginica' 

Pre-processing: centered (4), scaled (4) 
Resampling: Cross-Validated (10 fold, repeated 3 times) 
Summary of sample sizes: 100, 101, 100, 100, 101, 101, ... 
Resampling results across tuning parameters:

  k  Accuracy   Kappa    
  5  0.9496970  0.9242859
  7  0.9438384  0.9155406
  9  0.9590404  0.9386181

Accuracy was used to select the optimal model using  the largest value.
The final value used for the model was 9.
    </samp>
    </pre>
    
    <p>
       The terminal output above shows that our optimal <strong>k = 9</strong> based on the <code>Accuracy</code> and <code>Kappa</code> values.
    </p></section>
<section><h1>Prediction Results</h1>
    <p>Now we can plug in <strong>k = 9</strong> as one of the parameters into the <code>knn()</code> method. We also call our <code>knn.iris</code> model to see its output. Finally, we run the <code>CrossTable()</code> method to evaluate our model versus the test data.</p>
    <pre style="color:white"><samp>knn.iris <- knn(train = train.set, test = test.set, cl = class, k = 9, prob = T)

dim(train)
dim(class)
length(class)
table(test.class, knn.iris)

knn.iris

CrossTable(x = test.class, y = knn.iris, prop.chisq=FALSE)
    </samp>
    </pre>
<!-- [END] CODE HERE -->
<!-- CODE HERE -->
    
    <p>Below we can see our results of our model.</p>
<h2>Terminal Output</h2>    
    <pre><samp>> knn.iris <- knn(train = train.set, test = test.set, cl = class, k = 9, prob = T)

> dim(train)
[1] 112   5

> dim(class) # This is important since knn only registers the class componnent as a vector. You will get an error code if you use a data frame!!
NULL

> length(class)
[1] 112

> table(test.class, knn.iris)
knn.iris
test.class   setosa versicolor virginica
setosa         11          0         0
versicolor      0         13         0
virginica       0          1        13

> knn.iris
[1] setosa     setosa     setosa     setosa     setosa     setosa     setosa     setosa     setosa    
[10] setosa     setosa     versicolor versicolor versicolor versicolor versicolor versicolor versicolor
[19] versicolor versicolor versicolor versicolor versicolor versicolor virginica  virginica  virginica 
[28] versicolor virginica  virginica  virginica  virginica  virginica  virginica  virginica  virginica 
[37] virginica  virginica 
attr(,"prob")
[1] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
[11] 1.0000000 0.7692308 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
[21] 1.0000000 1.0000000 1.0000000 1.0000000 0.9090909 1.0000000 1.0000000 0.8181818 1.0000000 1.0000000
[31] 0.6363636 1.0000000 1.0000000 1.0000000 1.0000000 0.9090909 1.0000000 1.0000000
Levels: setosa versicolor virginica

> CrossTable(x = test.class, y = knn.iris, prop.chisq=FALSE)


Cell Contents
|-------------------------|
|                       N |
|           N / Row Total |
|           N / Col Total |
|         N / Table Total |
|-------------------------|


Total Observations in Table:  38 


 | knn.iris 
test.class |     setosa | versicolor |  virginica |  Row Total | 
-------------|------------|------------|------------|------------|
setosa |         11 |          0 |          0 |         11 | 
 |      1.000 |      0.000 |      0.000 |      0.289 | 
 |      1.000 |      0.000 |      0.000 |            | 
 |      0.289 |      0.000 |      0.000 |            | 
-------------|------------|------------|------------|------------|
versicolor |          0 |         13 |          0 |         13 | 
 |      0.000 |      1.000 |      0.000 |      0.342 | 
 |      0.000 |      0.929 |      0.000 |            | 
 |      0.000 |      0.342 |      0.000 |            | 
-------------|------------|------------|------------|------------|
virginica |          0 |          1 |         13 |         14 | 
 |      0.000 |      0.071 |      0.929 |      0.368 | 
 |      0.000 |      0.071 |      1.000 |            | 
 |      0.000 |      0.026 |      0.342 |            | 
-------------|------------|------------|------------|------------|
Column Total |         11 |         14 |         13 |         38 | 
 |      0.289 |      0.368 |      0.342 |            | 
-------------|------------|------------|------------|------------|
    </samp>
    </pre>
<p>We can see here that the model predicted virginica when it was actually versicolor which from our exploratory analysis we assumed there would be some prediction errors since these two species were the least distinguishable among all three species. Both tables are just reiterating the results, but we recieved one wrong prediction. Thus we calculate the test error as:</p>
<h2>Terminal Output</h2>
<pre><samp><strong><span style="color:#348589"># Total Number of observations is 38 
# Total Number of observations predicted incorrectly is 1</span></strong>
> 1/38
[1] 0.02631579</samp></pre>
<p>Thus we get a test error rate of 0.263158 which is not that bad, but granted this is a very easy set to use <code>Knn</code> modeling!</p>
</section>

<!-- 5. Conclusions-->
<section>
    <h1>Conclusions</h1>
    <p>Our model yielded <strong>test error rates is 0.263158</strong> for the three different species, not bad! As this project shows, <strong>K-Nearest Neighbors</strong> modeling is fairly simple. For datasets with a small amount of variables <strong>KNN</strong> is a viable method, but for datasets with many variables we run into the <em>curse of dimensionality</em>. Check this <a href="http://stats.stackexchange.com/questions/65379/machine-learning-curse-of-dimensionality-explained">stack exchange</a> article for an explanation on the <em>curse of dimensionality</em>. We feel like this project is a good intro to training and test sets which are very important components not just to data science but to statistical learning overall!</p>
</section>